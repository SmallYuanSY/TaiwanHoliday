#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°ç£æ”¿åºœè¡Œæ”¿æ©Ÿé—œè¾¦å…¬æ—¥æ›†è¡¨çˆ¬èŸ²
è‡ªå‹•å¾æ”¿åºœè³‡æ–™é–‹æ”¾å¹³è‡ºä¸‹è¼‰æœ€æ–°çš„è¾¦å…¬æ—¥æ›†è¡¨è³‡æ–™

Copyright (c) 2025 TaiwanHoliday
Licensed under the MIT License
"""

import requests
import os
import re
from datetime import datetime, timedelta
from typing import List, Dict
import logging
import urllib3

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TaiwanHolidayCrawler:
    def __init__(self):
        self.base_url = "https://data.gov.tw/dataset/14718"
        self.data_dir = "data"
        self.ensure_data_dir()
        
    def ensure_data_dir(self):
        """ç¢ºä¿è³‡æ–™ç›®éŒ„å­˜åœ¨"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            logger.info(f"å»ºç«‹è³‡æ–™ç›®éŒ„: {self.data_dir}")
    
    def get_available_years_and_urls(self) -> Dict[int, str]:
        """ç²å–ç¶²ç«™ä¸Šå¯ç”¨çš„å¹´ä»½æ¸…å–®åŠå…¶ä¸‹è¼‰é€£çµ"""
        available_data = {}
        try:
            response = requests.get(self.base_url, timeout=30, verify=False)
            response.raise_for_status()
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ä¸‹è¼‰é€£çµçš„å…ƒç´ 
            for link in soup.find_all('a', href=True):
                href = link['href']
                
                # æª¢æŸ¥æ˜¯å¦ç‚º dgpa.gov.tw çš„ FileConversion é€£çµ
                if 'dgpa.gov.tw/FileConversion' in href and 'name=' in href:
                    # è§£æURLåƒæ•¸ä¸­çš„æª”æ¡ˆåç¨±
                    import urllib.parse
                    parsed_url = urllib.parse.urlparse(href)
                    query_params = urllib.parse.parse_qs(parsed_url.query)
                    
                    if 'name' in query_params:
                        filename = query_params['name'][0]
                        
                        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å¾æª”æ¡ˆåç¨±ä¸­æå–å¹´ä»½
                        import re
                        pattern = r'(\d{3})å¹´ä¸­è¯æ°‘åœ‹æ”¿åºœè¡Œæ”¿æ©Ÿé—œè¾¦å…¬æ—¥æ›†è¡¨\.csv'
                        match = re.search(pattern, filename)
                        
                        if match:
                            roc_year = int(match.group(1))
                            western_year = roc_year + 1911
                            available_data[western_year] = href
                            logger.info(f"æ‰¾åˆ° {western_year} å¹´è³‡æ–™: {filename}")
                
                # ä¹Ÿæª¢æŸ¥é€£çµæ–‡å­—ä¸­çš„å¹´ä»½è³‡è¨Šä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ
                else:
                    link_text = link.get_text().strip()
                    import re
                    pattern = r'(\d{3})å¹´ä¸­è¯æ°‘åœ‹æ”¿åºœè¡Œæ”¿æ©Ÿé—œè¾¦å…¬æ—¥æ›†è¡¨'
                    match = re.search(pattern, link_text)
                    
                    if match and 'CSV' in link_text:
                        roc_year = int(match.group(1))
                        western_year = roc_year + 1911
                        
                        # åªæœ‰åœ¨é‚„æ²’æ‰¾åˆ°è©²å¹´ä»½è³‡æ–™æ™‚æ‰ä½¿ç”¨
                        if western_year not in available_data:
                            # è™•ç†é€£çµæ ¼å¼
                            if href.startswith('/'):
                                available_data[western_year] = f"https://data.gov.tw{href}"
                            elif href.startswith('http'):
                                available_data[western_year] = href
            
            logger.info(f"æ‰¾åˆ°å¯ç”¨å¹´ä»½åŠé€£çµ: {sorted(available_data.keys())}")
            return available_data
            
        except Exception as e:
            logger.error(f"ç²å–å¹´ä»½æ¸…å–®å¤±æ•—: {e}")
            return {}
    
    def get_available_years(self) -> List[int]:
        """ç²å–ç¶²ç«™ä¸Šå¯ç”¨çš„å¹´ä»½æ¸…å–®"""
        data = self.get_available_years_and_urls()
        years = list(data.keys())
        years.sort(reverse=True)  # ç”±æ–°åˆ°èˆŠæ’åº
        return years
    
    def get_csv_download_url(self, year: int) -> str:
        """æ ¹æ“šå¹´ä»½ç”¢ç”ŸCSVä¸‹è¼‰ç¶²å€"""
        roc_year = year - 1911  # è½‰æ›ç‚ºæ°‘åœ‹å¹´
        base_csv_url = "https://data.gov.tw/api/v1/rest/datastore_search?resource_id="
        
        # é€™è£¡éœ€è¦å¯¦éš›çš„resource_idï¼Œé€šå¸¸éœ€è¦å¾ç¶²é ä¸­è§£ææˆ–ä½¿ç”¨API
        # ç‚ºäº†ç¤ºç¯„ï¼Œæˆ‘å€‘ä½¿ç”¨ç›´æ¥æ§‹å»ºçš„æ–¹å¼
        csv_url = f"https://cloud.culture.tw/frontsite/trans/SearchShowAction.do?method=doFindTypeJ&category=6&ccid=&dtdid=221{roc_year:03d}"
        
        # å¯¦éš›æ‡‰è©²è¦è§£æç¶²é ä¸­çš„çœŸå¯¦ä¸‹è¼‰é€£çµ
        return self.parse_real_csv_url(year)
    
    def parse_real_csv_url(self, year: int) -> str:
        """å¾ç¶²é ä¸­è§£æçœŸå¯¦çš„CSVä¸‹è¼‰ç¶²å€"""
        try:
            response = requests.get(self.base_url, timeout=30, verify=False)
            response.raise_for_status()
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°‹æ‰¾å°æ‡‰å¹´ä»½çš„CSVé€£çµ
            roc_year = year - 1911
            pattern = f"{roc_year}å¹´ä¸­è¯æ°‘åœ‹æ”¿åºœè¡Œæ”¿æ©Ÿé—œè¾¦å…¬æ—¥æ›†è¡¨"
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ç›®æ¨™å¹´ä»½çš„é€£çµ
            for link in soup.find_all('a', href=True):
                link_text = link.get_text().strip()
                if pattern in link_text and 'CSV' in link_text:
                    href = link['href']
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚º dgpa.gov.tw çš„ FileConversion é€£çµ
                    if 'dgpa.gov.tw/FileConversion' in href:
                        logger.info(f"æ‰¾åˆ° {year} å¹´çš„ä¸‹è¼‰é€£çµ: {href}")
                        return href
                    
                    # è™•ç†å…¶ä»–å¯èƒ½çš„é€£çµæ ¼å¼
                    if href.startswith('/'):
                        full_url = f"https://data.gov.tw{href}"
                        logger.info(f"æ‰¾åˆ° {year} å¹´çš„ä¸‹è¼‰é€£çµ: {full_url}")
                        return full_url
                    elif href.startswith('http'):
                        logger.info(f"æ‰¾åˆ° {year} å¹´çš„ä¸‹è¼‰é€£çµ: {href}")
                        return href
            
            logger.warning(f"æ‰¾ä¸åˆ° {year} å¹´çš„CSVä¸‹è¼‰é€£çµ")
            return None
            
        except Exception as e:
            logger.error(f"è§£æCSVç¶²å€å¤±æ•—: {e}")
            return None
    
    def download_year_data(self, year: int) -> bool:
        """ä¸‹è¼‰æŒ‡å®šå¹´ä»½çš„è³‡æ–™"""
        try:
            # é¦–å…ˆå˜—è©¦å¾ç¶²é è§£æé€£çµ
            csv_url = self.parse_real_csv_url(year)
            
            # å¦‚æœè§£æå¤±æ•—ï¼Œä½¿ç”¨å·²çŸ¥çš„é€£çµä½œç‚ºå‚™ç”¨
            if not csv_url:
                known_urls = self.get_known_csv_urls()
                if year in known_urls:
                    csv_url = known_urls[year]
                    logger.info(f"ä½¿ç”¨å·²çŸ¥é€£çµä¸‹è¼‰ {year} å¹´è³‡æ–™")
                else:
                    logger.error(f"ç„¡æ³•å–å¾— {year} å¹´çš„ä¸‹è¼‰ç¶²å€")
                    return False
            
            logger.info(f"ä¸‹è¼‰ {year} å¹´è³‡æ–™: {csv_url}")
            
            response = requests.get(csv_url, timeout=30, verify=False)
            response.raise_for_status()
            
            # å„²å­˜åŸå§‹CSVæª”æ¡ˆ
            csv_filename = os.path.join(self.data_dir, f"taiwan_holidays_{year}.csv")
            
            # è™•ç†ç·¨ç¢¼å•é¡Œ
            content = response.content
            try:
                # å…ˆå˜—è©¦UTF-8
                text_content = content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    # å†å˜—è©¦Big5
                    text_content = content.decode('big5')
                except UnicodeDecodeError:
                    # æœ€å¾Œå˜—è©¦CP950
                    text_content = content.decode('cp950')
            
            with open(csv_filename, 'w', encoding='utf-8') as f:
                f.write(text_content)
            
            logger.info(f"æˆåŠŸä¸‹è¼‰ä¸¦å„²å­˜: {csv_filename}")
            
            # ç°¡å–®é©—è­‰è³‡æ–™ï¼ˆè¨ˆç®—è¡Œæ•¸ï¼‰
            with open(csv_filename, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                data_lines = len(lines) - 1  # æ¸›å»æ¨™é¡Œè¡Œ
                logger.info(f"{year} å¹´è³‡æ–™ç­†æ•¸: {data_lines}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è¼‰ {year} å¹´è³‡æ–™å¤±æ•—: {e}")
            return False
    
    def validate_csv_url(self, year: int, csv_url: str) -> bool:
        """é©—è­‰CSVé€£çµæ˜¯å¦ç¢ºå¯¦å°æ‡‰æŒ‡å®šå¹´ä»½"""
        try:
            # æª¢æŸ¥URLä¸­çš„nameåƒæ•¸
            if 'name=' in csv_url:
                import urllib.parse
                parsed_url = urllib.parse.urlparse(csv_url)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                
                if 'name' in query_params:
                    filename = query_params['name'][0]
                    roc_year = year - 1911
                    expected_pattern = f"{roc_year}å¹´ä¸­è¯æ°‘åœ‹æ”¿åºœè¡Œæ”¿æ©Ÿé—œè¾¦å…¬æ—¥æ›†è¡¨"
                    
                    if expected_pattern in filename:
                        logger.info(f"é€£çµé©—è­‰æˆåŠŸ: {filename}")
                        return True
                    else:
                        logger.warning(f"é€£çµå¹´ä»½ä¸ç¬¦: æœŸæœ› {roc_year}å¹´ï¼Œä½†æª”æ¡ˆåç‚º {filename}")
                        return False
            
            logger.warning(f"ç„¡æ³•å¾é€£çµä¸­é©—è­‰å¹´ä»½: {csv_url}")
            return True  # å¦‚æœç„¡æ³•é©—è­‰ï¼Œå‡è¨­é€£çµæœ‰æ•ˆ
            
        except Exception as e:
            logger.error(f"é©—è­‰é€£çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def download_year_data_direct(self, year: int, csv_url: str) -> bool:
        """ç›´æ¥ä½¿ç”¨æä¾›çš„URLä¸‹è¼‰æŒ‡å®šå¹´ä»½çš„è³‡æ–™"""
        try:
            # é¦–å…ˆé©—è­‰é€£çµ
            if not self.validate_csv_url(year, csv_url):
                logger.error(f"é€£çµé©—è­‰å¤±æ•—ï¼Œè·³éä¸‹è¼‰ {year} å¹´è³‡æ–™")
                return False
                
            logger.info(f"ä¸‹è¼‰ {year} å¹´è³‡æ–™: {csv_url}")
            
            response = requests.get(csv_url, timeout=30, verify=False)
            response.raise_for_status()
            
            # å„²å­˜åŸå§‹CSVæª”æ¡ˆ
            csv_filename = os.path.join(self.data_dir, f"taiwan_holidays_{year}.csv")
            
            # è™•ç†ç·¨ç¢¼å•é¡Œ
            content = response.content
            try:
                # å…ˆå˜—è©¦UTF-8
                text_content = content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    # å†å˜—è©¦Big5
                    text_content = content.decode('big5')
                except UnicodeDecodeError:
                    # æœ€å¾Œå˜—è©¦CP950
                    text_content = content.decode('cp950')
            
            with open(csv_filename, 'w', encoding='utf-8') as f:
                f.write(text_content)
            
            logger.info(f"æˆåŠŸä¸‹è¼‰ä¸¦å„²å­˜: {csv_filename}")
            
            # ç°¡å–®é©—è­‰è³‡æ–™ï¼ˆè¨ˆç®—è¡Œæ•¸ï¼‰
            with open(csv_filename, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                data_lines = len(lines) - 1  # æ¸›å»æ¨™é¡Œè¡Œ
                logger.info(f"{year} å¹´è³‡æ–™ç­†æ•¸: {data_lines}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è¼‰ {year} å¹´è³‡æ–™å¤±æ•—: {e}")
            return False
    
    def get_target_years(self, download_all: bool = False) -> List[int]:
        """å–å¾—ç›®æ¨™å¹´ä»½
        
        Args:
            download_all: æ˜¯å¦ä¸‹è¼‰æ‰€æœ‰å¯ç”¨å¹´ä»½ï¼ˆé è¨­ç‚ºFalseï¼Œåªä¸‹è¼‰ç•¶å‰å¹´å’Œä¸‹ä¸€å¹´ï¼‰
        """
        if download_all:
            # ä¸‹è¼‰æ‰€æœ‰å¯ç”¨å¹´ä»½
            available_years = self.get_available_years()
            if available_years:
                logger.info(f"ä¸‹è¼‰æ‰€æœ‰å¯ç”¨å¹´ä»½: {available_years}")
                return available_years
            else:
                logger.warning("ç„¡æ³•å–å¾—å¯ç”¨å¹´ä»½æ¸…å–®ï¼Œå›é€€åˆ°é è¨­æ¨¡å¼")
        
        # é è¨­æ¨¡å¼ï¼šåªä¸‹è¼‰ç•¶å‰å¹´ä»½å’Œä¸‹ä¸€å¹´ä»½
        current_year = datetime.now().year
        return [current_year, current_year + 1]
    
    def get_known_csv_urls(self) -> Dict[int, str]:
        """å·²çŸ¥çš„CSVä¸‹è¼‰é€£çµï¼ˆä½œç‚ºå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        return {
            # æ‰‹å‹•åŠ å…¥å·²çŸ¥çš„é€£çµä½œç‚ºæœ€å¾Œå‚™ç”¨æ–¹æ¡ˆ
            2021: "https://www.dgpa.gov.tw/FileConversion?filename=dgpa/files/202407/daec35f0-ae43-4466-aa26-bafb39aa8e0a.csv&nfix=&name=110%e4%b8%ad%e8%8f%af%e6%b0%91%e5%9c%8b%e6%94%bf%e5%ba%9c%e8%a1%8c%e6%94%bf%e6%a9%9f%e9%97%9c%e8%be%a6%e5%85%ac%e6%97%a5%e6%9b%86%e8%a1%a8.csv",
        }
    
    def run(self, download_all: bool = False):
        """åŸ·è¡Œçˆ¬èŸ²ä¸»ç¨‹å¼
        
        Args:
            download_all: æ˜¯å¦ä¸‹è¼‰æ‰€æœ‰å¯ç”¨å¹´ä»½ï¼ˆé è¨­ç‚ºFalseï¼‰
        """
        if download_all:
            logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå°ç£æ”¿åºœè¾¦å…¬æ—¥æ›†è¡¨çˆ¬èŸ² - ä¸‹è¼‰æ‰€æœ‰å¯ç”¨å¹´ä»½")
        else:
            logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå°ç£æ”¿åºœè¾¦å…¬æ—¥æ›†è¡¨çˆ¬èŸ² - ç¶­è­·æ¨¡å¼ï¼ˆç•¶å‰å¹´+ä¸‹ä¸€å¹´ï¼‰")
        
        # å–å¾—å¯ç”¨å¹´ä»½åŠå…¶ä¸‹è¼‰é€£çµ
        available_data = self.get_available_years_and_urls()
        if not available_data:
            logger.error("ç„¡æ³•å–å¾—å¯ç”¨å¹´ä»½æ¸…å–®")
            return
        
        # å–å¾—ç›®æ¨™å¹´ä»½
        target_years = self.get_target_years(download_all)
        logger.info(f"ç›®æ¨™å¹´ä»½: {target_years}")
        
        # ä¸‹è¼‰è³‡æ–™
        success_count = 0
        for year in target_years:
            if year in available_data:
                if self.download_year_data_direct(year, available_data[year]):
                    success_count += 1
            else:
                # å˜—è©¦ä½¿ç”¨å·²çŸ¥é€£çµä½œç‚ºå‚™ç”¨
                known_urls = self.get_known_csv_urls()
                if year in known_urls:
                    if self.download_year_data_direct(year, known_urls[year]):
                        success_count += 1
                else:
                    logger.warning(f"{year} å¹´çš„è³‡æ–™å°šæœªåœ¨ç¶²ç«™ä¸Šæä¾›")
        
        logger.info(f"çˆ¬èŸ²åŸ·è¡Œå®Œæˆï¼ŒæˆåŠŸä¸‹è¼‰ {success_count} å€‹å¹´ä»½çš„è³‡æ–™")
        
        # å¦‚æœæœ‰æˆåŠŸä¸‹è¼‰çš„æª”æ¡ˆï¼Œè½‰æ›ç‚º YAML æ ¼å¼
        if success_count > 0:
            self.convert_to_yaml()
        
        # é¡¯ç¤ºä¸‹è¼‰çš„æª”æ¡ˆ
        self.list_downloaded_files()
    
    def convert_to_yaml(self):
        """è½‰æ› CSV æª”æ¡ˆç‚º YAML æ ¼å¼"""
        try:
            from data_converter import HolidayDataConverter
            converter = HolidayDataConverter()
            converter.convert_all_csv_files()
            converter.create_summary_yaml()
            logger.info("å·²è‡ªå‹•è½‰æ›ç‚º YAML æ ¼å¼")
        except ImportError:
            logger.warning("ç„¡æ³•åŒ¯å…¥è³‡æ–™è½‰æ›å™¨ï¼Œè«‹ç¢ºä¿ data_converter.py æª”æ¡ˆå­˜åœ¨")
        except Exception as e:
            logger.error(f"è½‰æ› YAML æ ¼å¼å¤±æ•—: {e}")
    
    def list_downloaded_files(self):
        """åˆ—å‡ºå·²ä¸‹è¼‰çš„æª”æ¡ˆ"""
        if os.path.exists(self.data_dir):
            csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]
            yaml_files = [f for f in os.listdir(self.data_dir) if f.endswith('.yml')]
            
            if csv_files:
                logger.info("å·²ä¸‹è¼‰çš„ CSV æª”æ¡ˆ:")
                for file in sorted(csv_files):
                    file_path = os.path.join(self.data_dir, file)
                    file_size = os.path.getsize(file_path)
                    logger.info(f"  - {file} ({file_size} bytes)")
            
            if yaml_files:
                logger.info("å·²è½‰æ›çš„ YAML æª”æ¡ˆ:")
                for file in sorted(yaml_files):
                    file_path = os.path.join(self.data_dir, file)
                    file_size = os.path.getsize(file_path)
                    logger.info(f"  - {file} ({file_size} bytes)")
            
            if not csv_files and not yaml_files:
                logger.info("data ç›®éŒ„ä¸­æ²’æœ‰è³‡æ–™æª”æ¡ˆ")

def main():
    """ä¸»ç¨‹å¼é€²å…¥é»"""
    import sys
    
    # æª¢æŸ¥å‘½ä»¤åˆ—åƒæ•¸
    download_all = False
    if len(sys.argv) > 1 and sys.argv[1] == "--all":
        download_all = True
        print("ğŸ¯ åŸ·è¡Œæ¨¡å¼ï¼šä¸‹è¼‰æ‰€æœ‰å¯ç”¨å¹´ä»½")
    else:
        print("ğŸ¯ åŸ·è¡Œæ¨¡å¼ï¼šç¶­è­·æ¨¡å¼ï¼ˆç•¶å‰å¹´+ä¸‹ä¸€å¹´ï¼‰")
        print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --all åƒæ•¸å¯ä¸‹è¼‰æ‰€æœ‰å¯ç”¨å¹´ä»½")
    
    crawler = TaiwanHolidayCrawler()
    crawler.run(download_all)

if __name__ == "__main__":
    main() 